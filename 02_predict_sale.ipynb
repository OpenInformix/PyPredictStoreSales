{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World of Predict Store Sales\n",
    "\n",
    "This example is near replication of sample code published by DataBriefing for Kaggle Rossmann Store Sales competition. \n",
    "\n",
    "[Kaggle.com Rossmann Store Sales](https://www.kaggle.com/c/rossmann-store-sales/).\n",
    "\n",
    "[Original DataBriefing Code](https://github.com/databriefing/article-notebooks/tree/master/rossmann).\n",
    "[https://github.com/databriefing/article-notebooks/tree/master/rossmann](https://github.com/databriefing/article-notebooks/tree/master/rossmann).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('./data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IfxPy\n",
    "\n",
    "table_name = \"train1\"\n",
    "UserInform = 50000\n",
    "ConStr = \"SERVER=ids0;DATABASE=db1;HOST=127.0.0.1;SERVICE=9088;UID=informix;PWD=xxxxx;\"\n",
    "\n",
    "try:\n",
    "    # netstat -a | findstr  9088\n",
    "    conn = IfxPy.connect( ConStr, \"\", \"\")\n",
    "    IfxPy.autocommit(conn, IfxPy.SQL_AUTOCOMMIT_ON)\n",
    "except Exception as e:\n",
    "    print ('ERROR: Connect failed')\n",
    "    print ( e )\n",
    "    quit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sql = \"SELECT * FROM {}\".format(table_name)\n",
    "print( sql )\n",
    "\n",
    "stmt = IfxPy.exec_immediate(conn, sql)\n",
    "\n",
    "# Get a record as Python tuple\n",
    "tu = IfxPy.fetch_tuple(stmt)\n",
    "ls = []\n",
    "rc = 0\n",
    "while tu != False:\n",
    "    rc += 1\n",
    "    # Store the tuple in Python List\n",
    "    ls.append( tu )\n",
    "    if (rc % UserInform) == 0 :\n",
    "        print( \"Selected \", rc )\n",
    "    tu = IfxPy.fetch_tuple(stmt)\n",
    "\n",
    "# Convert the date to PandA DataFrame \n",
    "data = pd.DataFrame( ls, columns=[ \"Store\",\"DayOfWeek\",\"Date\",\"Sales\",\"Customers\",\"Open\",\"Promo\",\"StateHoliday\",\"SchoolHoliday\" ] )\n",
    "\n",
    "print(f'Number of Rows Selected is {rc} .')\n",
    "\n",
    "IfxPy.close(conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates descriptive statistics that summarize the central tendency,\n",
    "#  dispersion and shape of a datasetâ€™s distribution, excluding NaN values.\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print data types of the columns\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To list all different values of the StateHoliday column values\n",
    "data.StateHoliday.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that StateHoliday is no binary feature (0 or 1) it's not even a numeric feature.\n",
    "# This is a problem for most algorithms and so we'll have to fix this later on by creating dummy variables.\n",
    "# First let's fix an obvious mistake in the dataset: StateHoliday has both 0 as an integer and a string.\n",
    "# So let's convert this whole column to string values.\n",
    "data.StateHoliday = data.StateHoliday.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.StateHoliday.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the number of unique values for each colum\n",
    "# Apply the function to each column (axis=0)\n",
    "\n",
    "def count_unique(column):\n",
    "    return len(column.unique())\n",
    "\n",
    "data.apply(count_unique, axis=0).astype(np.int32)\n",
    "\n",
    "# We define a function and apply this function to each column (i.e. along axis 0)\n",
    "# This tells us a few interesting things.\n",
    "# Apparently there are over a thousand different stores and we have data for 942 different days.\n",
    "# Some features are binary and StateHoliday - as we've already seen - has 4 different values.\n",
    "# DayOfWeek unsurprisingly has 7 different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "# Missing values - most obvious when we have null values in\n",
    "# the dataset - are a huge problem and we'll focus on missing values in a future article.\n",
    "# Let's check if our dataset has any null values:\n",
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize store 150 sale data\n",
    "# Now would be a good moment to visualize some data. Just for intuition.\n",
    "# The following code takes sales numbers for a specific store - store 150 -\n",
    "# and plots the first 365 days sorted by Date.\n",
    "\n",
    "# Filter data for store 150 and plot sales data for first 365 days\n",
    "store_data = data[data.Store==150].sort_values('Date')\n",
    "plt.figure(figsize=(20, 10))  # Set figsize to increase size of figure\n",
    "plt.plot(store_data.Sales.values[:365])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can clearly see that this store is closed on Sundays.\n",
    "# But there's also an interesting pattern: Every second week or so sales increase.\n",
    "# Maybe we can find out why. Create a new cell, just input store_data and run the cell.\n",
    "# This will display the first rows of our store_data variable that holds all sales of store 150.\n",
    "# A feature that looks like it could correspond to that weekly period is Promo.\n",
    "\n",
    "# A great way to get an intuition for correlations is a scatter plot:\n",
    "# plt.figure(figsize=(20, 10))\n",
    "# plt.scatter(x=store_data[data.Open==1].Promo, y=store_data[data.Open==1].Sales, alpha=0.1)\n",
    "# plt.xlabel('Promo')\n",
    "# plt.ylabel('Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apparently sales are higher when they run a promo on the same day, which makes sense.\n",
    "# (To really, scientifically say something about the data we\n",
    "# would have to do some further analysis and statistical tests.\n",
    "# But we only want an intuition and try out some ways to visualize data so this will do for now.)\n",
    "# Now that we have a basic understanding of our dataset we can start to prepare it for prediction algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming Data by Dropping features\n",
    "# Let's think about the goal of our predictions:\n",
    "# We want to predict sales numbers for a specific day and store with a set of features that we know beforehand.\n",
    "# For example if we'll run a promo or what day of the week it will be.\n",
    "# We have a lot of features like these that should help the algorithm predict sales numbers.\n",
    "# But we also have three features in our data that don't make sense at this stage and so we'll drop them:\n",
    "\n",
    "#### Store:\n",
    "# The store number doesn't in itself predict sales. E.g. a higher store number says nothing about the sales.\n",
    "\n",
    "#### Date:\n",
    "# We could transform the date into something like days since first sale to catch a possible continuous sales growth but we don't do that now.\n",
    "\n",
    "#### Customers:\n",
    "# This column won't help us at all. As you can see in test.csv we won't have this feature later to make predictions.\n",
    "# Which is obvious as you don't know the number of customers on a given day in the future.\n",
    "# This would be a feature we could learn and predict just like sales numbers.\n",
    "\n",
    "transformed_data = data.drop(['Store', 'Date', 'Customers'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Categorical and Nominal Features\n",
    "# Let's look at StateHoliday again. In our dataset it has four unique values.\n",
    "# All of them strings: '0', 'a', 'b', 'c'.\n",
    "# To use this feature to train our algorithm we have to transform it into numerical values.\n",
    "# So could we instead just use 0, 1, 2, 3?\n",
    "# Not in this case and not in the case of DayOfWeek.\n",
    "# Like Store there is no intrinsic order, ranking or value in StateHoliday and\n",
    "# simply using numbers here would only confuse the algorithm\n",
    "\n",
    "# This replaces the feature with a binary feature for each value.\n",
    "# So for StateHoliday which can have the values 0, a, b or c it will replace\n",
    "# StateHoliday with StateHoliday_0, StateHoliday_a, StateHoliday_b and StateHoliday_c.\n",
    "# And for a row who's StateHoliday was b it would set StateHoliday_b = 1 and the other StateHoliday_ features = 0.\n",
    "# This technique is also called one-hot encoding\n",
    "# (because only the feature representing the value will be 1 - i.e. 'hot' - and the rest will be 0).\n",
    "\n",
    "transformed_data = pd.get_dummies(transformed_data, columns=['DayOfWeek', 'StateHoliday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we separate our dataset into the values we want to predict (Sales) and\n",
    "# the values to train the algorithm with (all our features like Promo, DayOfWeek_x, etc).\n",
    "X = transformed_data.drop(['Sales'], axis=1).values\n",
    "y = transformed_data.Sales.values\n",
    "\n",
    "# X is the matrix that contains all data from which we want to be able to predict sales data.\n",
    "# So before assigning the values of transformed_data to X we drop the Sales column.\n",
    "# .values finally gives us a matrix of raw values that we can feed to the algorithm.\n",
    "# y contains only the sales numbers.\n",
    "\n",
    "# The print statement shows us that X is a 1017209 by 14 matrix (14 features and 1017209 training examples).\n",
    "print(\"The training dataset has {} examples and {} features.\".format(X.shape[0], X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and cross-validating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us import the LinearRegression model of scikit-learn\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and cross_validation from scikit-learn\n",
    "from sklearn import cross_validation as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the LinearRegression model and KFold with 4 folds.\n",
    "# This splits our dataset into 4 parts. To ensure that the examples in these folds are\n",
    "# random we need to set shuffle=True. Remember, our dataset is sorted by date and store-ID so\n",
    "# without shuffle=True the first fold will contain the oldest data from stores with low IDs and so on.\n",
    "# We set the random_state to a specific value (in this case 42) just to get\n",
    "# consistent results when we rerun the training and testing.\n",
    "\n",
    "# We use our linear regression model lr, our dataset X, y and kfolds to run cross validation.\n",
    "\n",
    "# Finally cross_val_score runs cross validation four times\n",
    "# (because of our KFold with 4 folds) on our data and returns a list of these 4 scores\n",
    "lr = LinearRegression()\n",
    "kfolds = cv.KFold(X.shape[0], n_folds=4, shuffle=True, random_state=42)\n",
    "scores = cv.cross_val_score(lr, X, y, cv=kfolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing predictions\n",
    "\n",
    "We'll single out store 150 again and train our model on every store __except__ store 150 and then predict sales for store 150.\n",
    "\n",
    "Remember: __Always__ use different data for training and predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing predictions\n",
    "# In the middle of this article we've singled out store 150 and looked at the sales data for\n",
    "# the first 365 days. Now we'll train our algorithm on sales data from\n",
    "# all stores except store 150 (so we don't train and test with the same data) and\n",
    "# then predict sales numbers for store 150.\n",
    "\n",
    "# We'll single out store 150 again and train our model on every store except store 150 and then predict sales for store 150.\n",
    "# Remember: Always use different data for training and predicting.\n",
    "lr = LinearRegression()\n",
    "X_store = pd.get_dummies(data[data.Store!=150], columns=['DayOfWeek', 'StateHoliday']).drop(['Sales', 'Store', 'Date', 'Customers'], axis=1).values\n",
    "y_store = pd.get_dummies(data[data.Store!=150], columns=['DayOfWeek', 'StateHoliday']).Sales.values\n",
    "lr.fit(X_store, y_store)\n",
    "y_store_predict = lr.predict(pd.get_dummies(store_data, columns=['DayOfWeek', 'StateHoliday']).drop(['Sales', 'Store', 'Date', 'Customers'], axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both series in the same plot and see how well we did.\n",
    "plt.figure(figsize=(20, 10))  # Set figsize to increase size of figure\n",
    "plt.plot(store_data.Sales.values[:365], label=\"ground truth\")\n",
    "plt.plot(y_store_predict[:365], c='r', label=\"prediction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
